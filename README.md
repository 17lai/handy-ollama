# self-ollama
Learning to deploy Ollama with hands-on practice, making the deployment of large language models accessible to everyone!

## 项目简介
动手学Ollama，实现大模型本地化部署，快速在本地管理以及运行大模型，让CPU也可以玩转大模型部署！

## 立项理由
随着大模型的飞速发展，市面上出现了越来越多的开源大模型，但是许多模型的部署需要利用GPU资源，如何让大模型时代的红利普惠到每一个人，让每一个人都可以部署属于自己的大模型。Ollama是一个开源的大语言部署服务工具，只需CPU即可部署大模型。我们希望通过动手学Ollama这一开源教程，帮助学习者快速上手Ollama，让每一位大模型爱好者、学习者以及开发者都能在本地部署自己的大模型，进而开发一些大模型应用，让大模型赋能千行百业！

## 项目受众
- 希望不受GPU资源限制，在本地运行大模型；
- 希望在本地部署大模型，开发大模型应用；
- 希望在本地管理大模型，让本地模型安全可靠。

## 项目亮点
本项目旨在使用CPU部署本地大模型，虽然目前已经有很多LLM相关的教程，但是这些教程中模型基本上都需要GPU资源，这对于很多资源受限的学习者不是很友好。因此，本项目通过动手学Ollama，帮助学习者快速上手本地CPU部署大模型。

## 项目规划
### 目录（持续更新中...）
- [ ] Ollama介绍
- [ ] Ollama安装与配置
  - [ ] macOS
  - [ ] Windows
  - [ ] Linux
  - [ ] Docker
- [ ] Ollama导入模型到本地的三种方式
- [ ] 使用Ollama原生API开发应用
- [ ] LangChain中使用Ollama-Python
- [ ] LangChain中使用Ollama-JavaScript
- [ ] 基于 WebUI 部署 Ollama 可视化对话界面

