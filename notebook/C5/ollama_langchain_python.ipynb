{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ollama 在 LangChain 中的使用 - Python 集成\n",
   "id": "c0425f51befd6991"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. 环境设置\n",
    "### 运行前请确保已经配置好环境和依赖"
   ],
   "id": "1fc4457ed485339b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 确定当前环境\n",
    "import sys\n",
    "print(sys.executable)"
   ],
   "id": "8fbb985d29d84a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install langchain-ollama\n",
    "!pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install Pillow\n",
    "!pip install faiss-cpu"
   ],
   "id": "5902546ad74389aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.下载所需模型并初始化 OllamaLLM",
   "id": "e82331725aa6963d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 初始化 OllamaLLM",
   "id": "c88ce95d6c681681"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 初始化Ollama LLM\n",
    "model_name = \"llama3.1\"\n",
    "model = OllamaLLM(model=model_name)\n",
    "print(f\"OllamaLLM 初始化 {model_name} 完成\")"
   ],
   "id": "ed321fae89fc7a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 基本使用示例\n",
    "\n",
    "### 使用最基础的 ChatPromptTemplate 进行对话"
   ],
   "id": "f513b62f081aa0cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 创建一个简单的提示模版\n",
    "template = \"\"\"\n",
    "你是一个乐于助人的AI，擅长于解决回答各种问题。\n",
    "问题：{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# 创建一个简单的链\n",
    "chain = prompt | model\n",
    "# 使用链进行推理，输入问题\n",
    "chain.invoke({\"question\": \"你比GPT4厉害吗？\"})"
   ],
   "id": "9374f07909ce3ca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ChatPromptTemplate 允许我们创建一个可重用的模板，其中包含一个或多个参数。这些参数可以在运行时动态替换，以生成不同的提示。\n",
    "\n",
    "在创建链部分，使用管道操作符|，它将 prompt 和 model 连接起来，形成一个处理流程。这种链式操作使得我们可以轻松地组合和重用不同的组件。\n",
    "\n",
    "invoke 方法触发整个处理链，将我们的问题传入模板，然后将格式化后的提示发送给模型进行处理。"
   ],
   "id": "2d388fa5e6918b1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 流式输出\n",
    "流式输出是一种在生成长文本时逐步返回结果的技术。这种方法有几个重要的优势：\n",
    "\n",
    "1. 提高用户体验：用户可以立即看到部分结果，而不是等待整个响应完成。\n",
    "2. 减少等待时间：对于长回答，用户可以在完整回答生成之前就开始阅读。\n",
    "3. 实时交互：允许在生成过程中进行干预或终止。\n",
    "\n",
    "在实际应用中，特别是在聊天机器人或实时对话系统中，流式输出几乎是必不可少的。 \n",
    "\n",
    "下面的代码展示了如何使用 `model.stream()` 方法实现流式输出：\n"
   ],
   "id": "b8aa01fd1aa7142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 初始化ChatOllama模型\n",
    "model = ChatOllama(model=\"llama3.1\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    (\"human\", \"你好呀\"),\n",
    "]\n",
    "\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end='', flush=True)"
   ],
   "id": "ab3dc5835ebd15ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 工具调用  \n",
    "\n",
    "工具调用是 AI 模型与外部函数或 API 交互的能力。这使得模型可以执行复杂的任务，如数学计算、数据查询或外部服务调用。\n",
    "\n",
    "在这个例子中，我们定义了一个简单的计算器函数，并将其绑定到模型上：\n"
   ],
   "id": "8e5020b1edbad9f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def simple_calculator(operation: str, x: float, y: float) -> float:\n",
    "    if operation == \"add\":\n",
    "        return x + y\n",
    "    elif operation == \"subtract\":\n",
    "        return x - y\n",
    "    elif operation == \"multiply\":\n",
    "        return x * y\n",
    "    elif operation == \"divide\":\n",
    "        if y != 0:\n",
    "            return x / y\n",
    "        else:\n",
    "            raise ValueError(\"Cannot divide by zero\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation\")\n",
    "\n",
    "# 初始化绑定工具的 ChatOllama 模型\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ").bind_tools([simple_calculator])\n",
    "\n",
    "# 使用模型进行工具调用\n",
    "result = llm.invoke(\n",
    "    \"你知道一千万乘二是多少吗？\"\n",
    ")\n",
    "print(\"Tool calls:\", result.tool_calls)"
   ],
   "id": "73453e51383b4769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 多模态模型\n",
    "Ollama 支持多模态，LLMs例如 bakllava 和 llava.\n",
    "\n",
    "多模态模型是能够处理多种类型输入（如文本、图像、音频等）的 AI 模型。这些模型在理解和生成跨模态内容方面表现出色，使得更复杂和自然的人机交互成为可能。\n",
    "\n",
    "在我们的例子中，我们使用了支持图像和文本输入的 llava 模型："
   ],
   "id": "76c02c2a9e98eca0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  这一步是将图片转换为base64编码，以便后续多模态模型的使用\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    if pil_image.mode == 'RGBA':\n",
    "        pil_image = pil_image.convert('RGB')\n",
    "    pil_image.save(buffered, format=\"JPEG\")  \n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "file_path = \"../../docs/images/img-5-1-4.png\"  # 这里可以替换为你实际想要使用的图片路径\n",
    "pil_image = Image.open(file_path)\n",
    "\n",
    "image_b64 = convert_to_base64(pil_image)\n",
    "plt_img_base64(image_b64)"
   ],
   "id": "22bbbbba3165acb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llava\", temperature=0)\n",
    "\n",
    "def prompt_func(data):\n",
    "    text = data[\"text\"]\n",
    "    image = data[\"image\"]\n",
    "\n",
    "    image_part = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image}\",\n",
    "    }\n",
    "\n",
    "    content_parts = []\n",
    "\n",
    "    text_part = {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    content_parts.append(image_part)\n",
    "    content_parts.append(text_part)\n",
    "\n",
    "    return [HumanMessage(content=content_parts)]\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt_func | llm | StrOutputParser()\n",
    "\n",
    "query_chain = chain.invoke(\n",
    "    {\"text\": \"这个图片里是什么动物啊?\", \"image\": image_b64}\n",
    ")\n",
    "\n",
    "print(query_chain)"
   ],
   "id": "c7cc80f0795d55e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. 进阶用法\n",
    "\n",
    "### 使用 ConversationChain 进行对话\n",
    "\n",
    "`ConversationChain` 是 LangChain 提供的一个强大工具，用于管理多轮对话。它结合了语言模型、提示模板和内存组件，使得创建具有上下文感知能力的对话系统变得简单。\n"
   ],
   "id": "1f864c5340b17cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化Ollama LLM\n",
    "model_name = \"llama3.1\"\n",
    "model = OllamaLLM(model=model_name)\n",
    "\n",
    "# 初始化 ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 创建 ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ") # 设置 verbose=True 以显示调试信息, 默认为 False"
   ],
   "id": "80012ad0572f33c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这里的关键组件是：\n",
    "\n",
    "1. `ConversationBufferMemory`：这是一个简单的内存组件，它存储所有先前的对话历史。\n",
    "2. `ConversationChain`：它将语言模型、内存和一个默认的对话提示模板组合在一起。\n",
    "\n",
    "维护对话历史很重要，因为它允许模型：\n",
    "\n",
    "- 理解上下文和之前提到的信息\n",
    "- 生成更连贯和相关的回复\n",
    "- 处理复杂的多轮对话场景\n",
    "\n",
    "在实际应用中，你可能需要考虑使用更高级的内存组件，如 `ConversationSummaryMemory`，以处理长对话并避免超出模型的上下文长度限制。"
   ],
   "id": "dabe77655b407234"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在，让我们体验一下具有记忆的对话系统：",
   "id": "49c321373484b0f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第一轮对话\n",
    "response = conversation.predict(input=\"你好，我想了解一下人工智能。\")\n",
    "print(\"AI:\", response)\n",
    "\n",
    "# 第二轮对话\n",
    "response = conversation.predict(input=\"能给我举个AI在日常生活中的应用例子吗？\")\n",
    "print(\"AI:\", response)\n",
    "\n",
    "# 第三轮对话\n",
    "response = conversation.predict(input=\"这听起来很有趣。AI在医疗领域有什么应用？\")\n",
    "print(\"AI:\", response)"
   ],
   "id": "bd59cc268c82aa95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 自定义提示模板\n",
    "\n",
    "设计好的提示模板是创建高效 AI 应用的关键。在这个例子中，我们创建了一个用于生成产品描述的复杂提示："
   ],
   "id": "dc132dc39325e21b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# 初始化ChatOllama模型\n",
    "model = ChatOllama(model=\"llama3.1\", temperature=0.7)"
   ],
   "id": "c113ecc30ee7a978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_message = SystemMessage(content=\"\"\"\n",
    "你是一位经验丰富的电商文案撰写专家。你的任务是根据给定的产品信息创作吸引人的商品描述。\n",
    "请确保你的描述简洁、有力，并且突出产品的核心优势。\n",
    "\"\"\")\n",
    "\n",
    "human_message_template = \"\"\"\n",
    "请为以下产品创作一段吸引人的商品描述：\n",
    "产品类型: {product_type}\n",
    "核心特性: {key_feature}\n",
    "目标受众: {target_audience}\n",
    "价格区间: {price_range}\n",
    "品牌定位: {brand_positioning}\n",
    "\n",
    "请提供以下三种不同风格的描述，每种大约50字：\n",
    "1. 理性分析型\n",
    "2. 情感诉求型\n",
    "3. 故事化营销型\n",
    "\"\"\"\n",
    "\n",
    "def generate_product_descriptions(product_info):\n",
    "    human_message = HumanMessage(content=human_message_template.format(**product_info))\n",
    "    messages = [system_message, human_message]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n"
   ],
   "id": "2601aaba115cb0bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 示例使用\n",
    "product_info = {\n",
    "    \"product_type\": \"智能手表\",\n",
    "    \"key_feature\": \"心率监测和睡眠分析\",\n",
    "    \"target_audience\": \"注重健康的年轻专业人士\",\n",
    "    \"price_range\": \"中高端\",\n",
    "    \"brand_positioning\": \"科技与健康的完美结合\"\n",
    "}\n",
    "\n",
    "result = generate_product_descriptions(product_info)\n",
    "print(result)"
   ],
   "id": "16391b088e8d7665",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这个结构有几个重要的设计考虑：\n",
    "\n",
    "1. system_prompt：定义了 AI 的角色和总体任务，设置了整个对话的基调。\n",
    "2. human_message_template：提供了具体的指令和所需信息的结构。\n",
    "3. 多参数设计：允许灵活地适应不同的产品和需求。\n",
    "4. 多样化输出要求：通过要求不同风格的描述，鼓励模型展示其versatility。\n",
    "\n",
    "设计有效的提示模板时，考虑以下几点：\n",
    "\n",
    "- 明确定义 AI 的角色和任务\n",
    "- 提供清晰、结构化的输入格式\n",
    "- 包含具体的输出要求和格式指导\n",
    "- 考虑如何最大化模型的能力和创造力"
   ],
   "id": "f1a6a0ad942fb8d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 构建一个简单的RAG问答系统  \n",
    "RAG（Retrieval-Augmented Generation）是一种结合了检索和生成的AI技术，它通过检索相关信息来增强语言模型的回答能力。  \n",
    "RAG的优势在于它可以帮助语言模型访问最新和专业的信息，减少幻觉，并提高回答的准确性和相关性。  \n",
    "  \n",
    "LangChain 提供了多种组件，可以与 Ollama 模型无缝集成。这里我们将展示如何将 Ollama 模型与向量存储和检索器结合使用，创建一个简单的RAG问答系统。\n",
    "\n",
    "首先需要确保下载embedding模型，可以在命令行执行以下命令：  \n",
    "\n",
    "```shell\n",
    "ollama pull nomic-embed-text"
   ],
   "id": "8a9f8be9507d676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 初始化 Ollama 模型和嵌入\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# 准备文档\n",
    "text = \"\"\"\n",
    "Datawhale 是一个专注于数据科学与 AI 领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。\n",
    "Datawhale 以“ for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。\n",
    "同时 Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。\n",
    "如果你想在Datawhale开源社区发起一个开源项目，请详细阅读Datawhale开源项目指南[https://github.com/datawhalechina/DOPMC/blob/main/GUIDE.md]\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# 创建向量存储\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 创建提示模板\n",
    "template = \"\"\"只能使用下列内容回答问题:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 创建检索-问答链\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# 使用链回答问题\n",
    "question = \"我想为datawhale贡献该怎么做？\"\n",
    "response = chain.invoke(question)\n",
    "print(response.content)"
   ],
   "id": "3c848e6d6ea912cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fcb19eb70ac1303f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handlm",
   "language": "python",
   "name": "handlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
