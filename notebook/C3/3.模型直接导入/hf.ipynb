{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载模型\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"Qwen/Qwen1.5-0.5B\" \n",
    "snapshot_download(\n",
    "    repo_id=model_id, \n",
    "    local_dir=\"Qwen-0.5b\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=\"<YOUR_ACCESS_TOKEN>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用llama.cpp库进行转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 克隆仓库  终端执行以下命令，与下载的模型在同一目录下\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.安装依赖并测试\n",
    "%cd llama.cpp\n",
    "%pip install -r requirements.txt\n",
    "!pip install huggingface_hub\n",
    "%python convert.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.转换模型格式脚本\n",
    "!python convert_hf_to_gguf.py ../Qwen-0.5b --outfile Qwen_instruct_0.5b.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.模型量化\n",
    "!ollama create -q Q4_K_M mymodel3 -f ./Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传至huggingface repo\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "HF_ACCESS_TOKEN = \"<YOUR_ACCESS_TOKEN>\"\n",
    "#TODO 这里需要设置你的model_id\n",
    "model_id = \"little1d/QWEN-0.5b\"\n",
    "\n",
    "api.create_repo(\n",
    "    model_id,\n",
    "    exist_ok=True,\n",
    "    repo_type=\"model\", # 上傳格式為模型\n",
    "    use_auth_token=HF_ACCESS_TOKEN,\n",
    ")\n",
    "# upload the model to the hub\n",
    "# upload model name includes the Bailong-instruct-7B in same folder\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".gguf\"):\n",
    "        model_name = file.lower()\n",
    "        api.upload_file(\n",
    "            repo_id=model_id,\n",
    "            path_in_repo=model_name,\n",
    "            path_or_fileobj=f\"{os.getcwd()}/{file}\",\n",
    "            repo_type=\"model\", # 上傳格式為模型\n",
    "            use_auth_token='<YOUR_ACCESS_TOKEN>')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
