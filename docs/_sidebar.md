* 目录
    * [第 1 章 Ollama 介绍](C1/1.%20Ollama%20介绍.md)
    * 第 2 章 Ollama 安装与配置
        * [2.1 macOS](C2/1.%20Ollama%20在%20macOS%20下的安装与配置.md)
        * [2.2 Windows](C2/2.%20Ollama%20在%20Windows%20下的安装与配置.md)
        * [2.3 Linux](C2/3.%20Ollama%20在%20Linux%20下的安装与配置.md)
        * [2.4 Docker](C2/4.%20Ollama%20在%20Docker%20下的安装与配置.md)
    * 第 3 章 自定义使用 Ollama
        * [3.1 自定义导入模型](C3/1.%20自定义导入模型.md)
        * [3.2 自定义模型存储位置](C3/2.%20自定义模型存储位置.md)
        * [3.3 自定义在 GPU 中运行](C3/3.%20自定义在%20GPU%20中运行.md)
    * 第 4 章 Ollama REST API
        * [4.1 Ollama API 使用指南](C4/1.%20Ollama%20API%20使用指南.md)
        * [4.2 在 Python 中使用 Ollama API](C4/2.%20在%20Python%20中使用%20Ollama%20API.md)
        * [4.3 在 Java 中使用 Ollama API](C4/3.%20在%20Java%20中使用%20Ollama%20API.md)
        * [4.4 在 JavaScript 中使用 Ollama API](C4/4.%20在%20JavaScript%20中使用%20Ollama%20API.md)
        * [4.5 在 C++ 中使用 Ollama API](C4/5.%20在%20C++%20中使用%20Ollama%20API.md)
    * 第 5 章 Ollama 在 LangChain 中的使用
        * [5.1 在 Python 中的集成](C5/1.%20Ollama%20在%20LangChain%20中的使用%20-%20Python%20集成.md)
        * [5.2 在 JavaScript 中的集成](C5/2.%20Ollama%20在%20LangChain%20中的使用%20-%20JavaScript%20集成.md)
    * 第 6 章 Ollama 可视化界面部署
        * [6.1 使用 FastAPI 部署 Ollama 可视化对话界面](C6/1.%20使用%20FastAPI%20部署%20Ollama%20可视化对话界面.md)
        * [6.2 使用 WebUI 部署 Ollama 可视化对话界面](C6/2.%20使用%20WebUI%20部署%20Ollama%20可视化对话界面.md)
    * 第 7 章 应用案例
        * [7.1 搭建本地的 AI Copilot 编程助手](C7/1.%20搭建本地的%20AI%20Copilot%20编程助手.md)
        * [7.2 Dify 接入 Ollama 部署的本地模型](C7/2.%20Dify%20接入%20Ollama%20部署的本地模型.md)
        * [7.3 使用 LangChain 搭建本地 RAG 应用](C7/3.%20使用%20LangChain%20搭建本地%20RAG%20应用.md)
        * [7.4 使用 LlamaIndex 搭建本地 RAG 应用](C7/4.%20使用%20LlamaIndex%20搭建本地%20RAG%20应用.md)
        * [7.5 使用 LangChain 实现本地 Agent](C7/5.%20使用%20LangChain%20实现本地%20Agent.md)
        * [7.6 使用 LlamaIndex 实现本地 Agent](C7/6.%20使用%20LlamaIndex%20实现本地%20Agent.md)
        * [7.7 使用 DeepSeek R1 和 Ollama 实现本地 RAG 应用](C7/7.%20使用%20DeepSeek%20R1%20和%20Ollama%20实现本地%20RAG%20应用.md)