# 在 C++ 中使用 Ollama API

本文介绍了如何在 C++ 中使用 Ollama API 。这篇文档旨在帮助 C++ 开发者快速上手并充分利用 Ollama 的能力。通过学习本文档，你可以轻松集成 Ollama 到你的项目中。

>需要注意的是，Ollama 的社区和文档可能更侧重于 Python 和其他高级语言的集成。在 C++ 项目中使用大模型，通常会使用 Llama.cpp [(llama.cpp 教程)](https://www.datacamp.com/tutorial/llama-cpp-tutorial) 。Llama.cpp 是一个由 Georgi Gerganov 开发的开源项目，它提供了 Meta 的 LLaMa 架构的高效 C/C++ 实现，专注于大型语言模型（LLM）的推理过程。

## 一、 环境准备

关于 C++ 项目的开发环境这里不再赘述，这里只介绍如何集成 Ollama。请确保你已经安装并运行了 Ollama。如果你还没有安装 Ollama，请先安装并运行 Ollama，并下载所需要的模型（参考第二章内容）。

## 二、调用 Ollama API
直接调用 Ollama API 比较繁琐，这里介绍两种思路，1. 使用封装好的 C++ 类库，2. 通过 Python 调用 Ollama 的模型。

### 1. 使用封装好的 C++ 类库
在 Github 可以找到封装好的 C++ 类库，只需要引入头文件即可使用，避免重复造轮子。这里以 [ollama-hpp](https://github.com/jmont-dev/ollama-hpp) 为例：

```C++
#include "ollama.hpp"

std::cout << ollama::generate("llama3.1:latest", "how to use ollama in a cpp project?") << std::endl;
```
目前，Github 上封装好的 C++ 类库只有这一个，在测试过程中也存在一些 bug 需要手动修复，不建议使用这种方法。

### 2. 通过 Python 调用 Ollama 的模型
在 C++ 项目中集成 Ollama 的功能，可以通过 Python 调用 Ollama 的模型，再将结果传递给 C++ 程序：

1. 编写 Python 代码调用 Ollama API：使用 Python 编写代码，通过 Ollama 提供的 API 接口调用所需的模型生成文本或进行对话。以下是一个简单的 Python 调用示例：

    ```python
    import requests

    def call_ollama_api(prompt, model_name="llama3.1:latest"):
        url = "http://localhost:11434/api/generate"
        headers = {"Content-Type": "application/json"}
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False
        }
        response = requests.post(url, json=payload, headers=headers)
        return response.json()
        # 示例调用
    response = call_ollama_api("黑神话好玩吗？")
    print(response)
    ```

2. 从 C++ 调用 Python 脚本：在 C++ 程序中，可以使用系统调用来执行 Python 脚本，并将所需的文本或问题作为参数传递给 Python 脚本。例如，在 C++ 中可以使用以下方式调用 Python 脚本：
    ```cpp
    #include <cstdlib>

    int main() {
        system("python call_ollama.py '黑神话好玩吗？'");
        return 0;
    }
    ```

> 参考文档
 * [ollama-hpp](https://github.com/jmont-dev/ollama-hpp)
 * [llama.cpp 教程](https://www.datacamp.com/tutorial/llama-cpp-tutorial)
