
# 在 Python 中使用 Ollama API

在本文中，我们将简单介绍如何在 Python 中使用 Ollama API。无论你是想进行简单的聊天对话、使用流式响应处理大数据、还是希望在本地进行模型的创建、复制、删除等操作，本文都可以为你提供指导。此外，我们还展示了如何使用自定义客户端和异步编程来优化你的应用程序性能，无论你是 Ollama 的新手还是经验丰富的开发者，本博客都能帮助你在 Python 中更高效地使用Ollama API。

> 本教程还为大家提供了一个[Jupyter nodebook](../../notebook/C4/%E5%9C%A8%20Python%20%E4%B8%AD%E4%BD%BF%E7%94%A8%20Ollama%20API.ipynb)示例，来让大家更好的学习。
## 环境准备
在开始使用 Python 与 Ollama API 交互之前，请确保您的开发环境满足以下条件：

* Python: 安装 Python 3.8 或更高版本。
* pip: 确保已安装 pip，Python 的包管理工具。
* ollama 库: 用于更方便地与 Ollama API 交互。安装命令如下：

```sh
pip install ollama
```

## 使用方法

```python
import ollama
response = ollama.chat(model='llama3.1', messages=[
  {
    'role': 'user',
    'content': '为什么天空是蓝色的？',
  },
])
print(response['message']['content'])
```

## 流式响应

可以通过设置 `stream=True` 启用响应流，使函数调用返回一个 Python 生成器，其中每个部分都是流中的一个对象。

```python
import ollama

stream = ollama.chat(
    model='llama3.1',
    messages=[{'role': 'user', 'content': '为什么天空是蓝色的？'}],
    stream=True,
)

for chunk in stream:
  print(chunk['message']['content'], end='', flush=True)
```

## API

Ollama Python库提供了丰富的接口，简化了与Ollama的交互。这些接口设计直观，易于集成，旨在帮助开发者更便捷地调用和管理模型。如果你想了解更详细的底层实现和完整的API端点信息，建议参考[Ollama API 使用指南](./Ollama%20API%20使用指南.md)。


### 聊天

```python
ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': '为什么天空是蓝色的？'}])
```

### 生成

```python
ollama.generate(model='llama3.1', prompt='为什么天空是蓝色的？')
```

### 本地模型列表

```python
ollama.list()
```

### 显示模型信息

```python
ollama.show('llama3.1')
```

### 创建模型

```python
modelfile='''
FROM llama3.1
SYSTEM 你是超级马里奥兄弟中的马里奥。
'''

ollama.create(model='example', modelfile=modelfile)
```

### 复制模型

```python
ollama.copy('llama3.1', 'user/llama3.1')
```

### 删除模型

```python
ollama.delete('llama3.1')
```

### 拉取模型

```python
ollama.pull('llama3.1')
```

### 推送模型

```python
ollama.push('user/llama3.1')
```

### 生成嵌入

```python
ollama.embeddings(model='llama3.1', prompt='天空是蓝色的因为瑞利散射')
```

### 进程

```python
ollama.ps()
```

## 自定义客户端

可以使用以下字段创建自定义客户端：

- `host`: 要连接的 Ollama 主机
- `timeout`: 请求超时时间

```python
from ollama import Client
client = Client(host='http://localhost:11434')
response = client.chat(model='llama3.1', messages=[
  {
    'role': 'user',
    'content': '为什么天空是蓝色的？',
  },
])
```

## 异步客户端

```python
import asyncio
from ollama import AsyncClient
import nest_asyncio

nest_asyncio.apply()

async def chat():
    message = {'role': 'user', 'content': '为什么天空是蓝色的？'}
    response = await AsyncClient().chat(model='llama3.1', messages=[message])
    print(response)

asyncio.run(chat())

```

设置 `stream=True` 修改函数以返回 Python 异步生成器：

```python
import asyncio
from ollama import AsyncClient
import nest_asyncio

nest_asyncio.apply()
async def chat():
  message = {'role': 'user', 'content': '为什么天空是蓝色的？'}
  async for part in await AsyncClient().chat(model='llama3.1', messages=[message], stream=True):
    print(part['message']['content'], end='', flush=True)

asyncio.run(chat())
```

## 错误

如果请求返回错误状态或在流式传输时检测到错误，则会引发错误。

```python
model = 'does-not-yet-exist'

try:
  ollama.chat(model)
except ollama.ResponseError as e:
  print('错误:', e.error)
  if e.status_code == 404:
    ollama.pull(model)
```
> 参考文档
- [Ollama python](https://github.com/ollama/ollama-python)
